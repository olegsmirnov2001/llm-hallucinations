{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0143f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_black\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c011d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "os.chdir(project_root)\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2005676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tqdm_streamer import TokenProgressStreamer\n",
    "from lib.soft_stop_thinking import get_soft_stop_thinking_fn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import httpx\n",
    "from openai import OpenAI\n",
    "from typing import Sequence\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7545e0",
   "metadata": {},
   "source": [
    "# Generate Qwen responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05801951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7622501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 101.97172224)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    torch.cuda.memory_allocated() / 1e9,\n",
    "    torch.cuda.memory_reserved() / 1e9,\n",
    "    torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492c65bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:00<00:00, 450.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 5120)\n",
       "    (layers): ModuleList(\n",
       "      (0-63): 64 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=8192, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=8192, out_features=5120, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=25600, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=25600, bias=False)\n",
       "          (down_proj): Linear(in_features=25600, out_features=5120, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((5120,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'Qwen/Qwen3-32B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "model.to(device)  # type: ignore\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e781b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_THINK_TOKEN_ID = tokenizer.added_tokens_encoder.get(\"</think>\")\n",
    "\n",
    "\n",
    "def generate_with_logits(\n",
    "    prompt: str,\n",
    "    enable_thinking: bool = True,\n",
    "    max_new_tokens: int = 32768,\n",
    "    thinking_budget: int = 32768,\n",
    "    temperature: float = 0.6,  # recommended 0.7 for non-thinking mode\n",
    "    top_p: float = 0.95,  # recommended 0.8 for non-thinking mode\n",
    "    top_k: int = 20,\n",
    "    random_state: int | None = None,\n",
    "    samples: int = 1,\n",
    ") -> list[tuple[np.ndarray, list[int], str, str | None, int]]:\n",
    "    '''\n",
    "    Generate text from a prompt and return final logit activations.\n",
    "\n",
    "    Args:\n",
    "        prompt: The user prompt to generate from\n",
    "        enable_thinking: Whether to enable thinking mode (default True)\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Sampling temperature (default 0.6 for thinking, 0.7 for non-thinking)\n",
    "        top_p: Top-p sampling (default 0.95 for thinking, 0.8 for non-thinking)\n",
    "        top_k: Top-k sampling (default 20)\n",
    "\n",
    "    Returns:\n",
    "        tuple of (logits tensor, final content, thinking content or None, thinking duration)\n",
    "    '''\n",
    "    if thinking_budget > max_new_tokens:\n",
    "        thinking_budget = max_new_tokens\n",
    "\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text] * samples, return_tensors='pt').to(device)\n",
    "\n",
    "    input_token_length = model_inputs.input_ids.shape[-1]\n",
    "    streamer = TokenProgressStreamer(max_new_tokens=max_new_tokens)\n",
    "    prefix_allowed_tokens_fn = (\n",
    "        get_soft_stop_thinking_fn(\n",
    "            tokenizer=tokenizer,\n",
    "            thinking_budget=thinking_budget,\n",
    "            input_length=input_token_length,\n",
    "            model_vocab_size=model.config.vocab_size,\n",
    "        )\n",
    "        if enable_thinking\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            streamer=streamer,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        )\n",
    "\n",
    "    all_scores = torch.stack(outputs.scores, dim=0)  # type: ignore[union-attr] # (samples, num_generated_tokens, vocab_size)\n",
    "\n",
    "    # todo: vectorize this\n",
    "    def extract_sample(sample_index: int) -> tuple[np.ndarray, list[int], str, str | None, int]:\n",
    "        generated_ids = outputs.sequences[sample_index][input_token_length:].tolist()  # type: ignore[union-attr]\n",
    "        logits = all_scores[:, sample_index, :].cpu().numpy()\n",
    "\n",
    "        thinking_content = None\n",
    "        content = ''\n",
    "        thinking_duration = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            try:\n",
    "                index = len(generated_ids) - generated_ids[::-1].index(END_THINK_TOKEN_ID)\n",
    "                thinking_duration = index + 1\n",
    "                thinking_content = tokenizer.decode(generated_ids[:index], skip_special_tokens=True).strip()\n",
    "                content = tokenizer.decode(generated_ids[index:], skip_special_tokens=True).strip()\n",
    "            except ValueError:\n",
    "                print(f'No </think> token found for sample {sample_index} for prompt {prompt!r}')\n",
    "                thinking_duration = len(generated_ids)\n",
    "                thinking_content = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "        else:\n",
    "            content = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        return logits, generated_ids, content, thinking_content, thinking_duration\n",
    "\n",
    "    return [extract_sample(i) for i in range(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_quote_knowledge(\n",
    "    quote: str,\n",
    "    enable_thinking: bool = True,\n",
    "    random_state: int | None = None,\n",
    "    max_new_tokens: int = 2048,\n",
    "    samples: int = 1,\n",
    ") -> list[dict]:\n",
    "    prompt = (\n",
    "        f'\"{quote}\" Do you know where is the phrase from? '\n",
    "        'If you don\\'t know, say \"No, origin is unclear.\". If you know, say \"Yes, the phrase is from <source>.\".'\n",
    "    )\n",
    "    thinking_budget = max_new_tokens - 128 if enable_thinking else 0\n",
    "\n",
    "    generated_samples = generate_with_logits(\n",
    "        prompt=prompt,\n",
    "        enable_thinking=enable_thinking,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        thinking_budget=thinking_budget,\n",
    "        top_p=1.0,\n",
    "        random_state=random_state,\n",
    "        samples=samples,\n",
    "    )\n",
    "\n",
    "    def get_info(\n",
    "        sample_index: int,\n",
    "        logits: np.ndarray,\n",
    "        generated_ids: list[int],\n",
    "        content: str,\n",
    "        thinking_content: str | None,\n",
    "        thinking_duration: int,\n",
    "    ):\n",
    "        thinking_completed = thinking_duration < thinking_budget\n",
    "\n",
    "        YES = tokenizer.vocab['Yes']\n",
    "        NO = tokenizer.vocab['No']\n",
    "\n",
    "        position = next(\n",
    "            (pos for pos in range(thinking_duration, max_new_tokens) if generated_ids[pos] in [YES, NO]), None\n",
    "        )\n",
    "        if position is None:\n",
    "            print('No answer found')\n",
    "            return {\n",
    "                'quote': quote,\n",
    "                'sample_index': sample_index,\n",
    "                'random_state': random_state,\n",
    "                'enable_thinking': enable_thinking,\n",
    "                'thinking_completed': thinking_completed,\n",
    "                'thinking_duration': thinking_duration,\n",
    "                'max_new_tokens': max_new_tokens,\n",
    "                'content': content,\n",
    "                'thinking_content': thinking_content,\n",
    "            }\n",
    "\n",
    "        yes_logit = logits[position][YES].item()\n",
    "        no_logit = logits[position][NO].item()\n",
    "\n",
    "        positive_difference = yes_logit - no_logit\n",
    "        positive_probability = 1 / (1 + np.exp(-positive_difference))\n",
    "\n",
    "        return {\n",
    "            'quote': quote,\n",
    "            'sample_index': sample_index,\n",
    "            'positive_probability': positive_probability,\n",
    "            'positive_difference': positive_difference,\n",
    "            'yes_logit': yes_logit,\n",
    "            'no_logit': no_logit,\n",
    "            'random_state': random_state,\n",
    "            'enable_thinking': enable_thinking,\n",
    "            'thinking_completed': thinking_completed,\n",
    "            'thinking_duration': thinking_duration,\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'content': content,\n",
    "            'thinking_content': thinking_content,\n",
    "        }\n",
    "\n",
    "    return [get_info(index, *sample) for index, sample in enumerate(generated_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fbd10",
   "metadata": {},
   "source": [
    "# Test responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2a83c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using proxy: None\n"
     ]
    }
   ],
   "source": [
    "PROXY = os.environ.get('PROXY')\n",
    "print('Using proxy:', PROXY)\n",
    "\n",
    "httpx_client = (\n",
    "    httpx.Client(\n",
    "        proxy=PROXY,\n",
    "        timeout=30.0,\n",
    "    )\n",
    "    if PROXY\n",
    "    else None\n",
    ")\n",
    "client = OpenAI(\n",
    "    base_url='https://openrouter.ai/api/v1',\n",
    "    api_key=os.environ['OPENROUTER_API_KEY'],\n",
    "    http_client=httpx_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ccebb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(system_prompt: str, prompt: str, max_tokens: int = 100) -> str:\n",
    "    global messages\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt,\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model='anthropic/claude-haiku-4.5',\n",
    "        messages=messages,  # type: ignore\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    assert len(completion.choices) == 1\n",
    "    choice = completion.choices[0]\n",
    "\n",
    "    assert choice.message.content is not None\n",
    "    return choice.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ef800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_answers(reference: str, answers: Sequence[str]) -> list[dict[str, bool]]:\n",
    "    attempted = [index for index, answer in enumerate(answers) if answer.startswith('Yes')]\n",
    "\n",
    "    system_prompt = textwrap.dedent(\n",
    "        '''\n",
    "        Check the answers of a quiz. Participants have to guess the origin of a phrase.\n",
    "        For each answer, compare it with the reference and write \"Correct\" or \"Wrong\".\n",
    "        Accept any answer that mentions the reference. Just one word: \"Correct\" or \"Wrong\".\n",
    "\n",
    "        # Example\n",
    "\n",
    "        Input:\n",
    "        Reference: \"The Hangover\".\n",
    "        Participants answered:\n",
    "        1. it is from the Hangover movie.\n",
    "        2. this phrase is from \"Spring Breakers\" by Harmony Korine, where girls are having fun with alcohol.\n",
    "        3. the quote is from a famous 21st century comedy movie.\n",
    "\n",
    "        Output:\n",
    "        1. Correct\n",
    "        2. Wrong\n",
    "        3. Wrong\n",
    "        '''\n",
    "    ).strip()\n",
    "\n",
    "    def shorten_if_needed(answer_body: str) -> str:\n",
    "        N = 150\n",
    "        if len(answer_body) <= N:\n",
    "            return answer_body\n",
    "        return answer_body[: N - 3] + '...'\n",
    "\n",
    "    prompt = textwrap.dedent(\n",
    "        '''\n",
    "        Reference: \"{reference}\".\n",
    "        Participants answered:\n",
    "        {answers}\n",
    "        '''\n",
    "    ).format(\n",
    "        reference=reference,\n",
    "        answers='\\n'.join(\n",
    "            '{i}. {answer_body}'.format(\n",
    "                i=number + 1,\n",
    "                answer_body=shorten_if_needed(answers[index].removeprefix('Yes').lstrip(', ')),\n",
    "            )\n",
    "            for number, index in enumerate(attempted)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    global llm_grading\n",
    "    llm_grading = get_llm_response(system_prompt=system_prompt, prompt=prompt, max_tokens=5 * len(attempted))\n",
    "    llm_grading_lines = llm_grading.split('\\n')\n",
    "\n",
    "    def process_answer(index, answer) -> dict[str, bool]:\n",
    "        if not answer.startswith('Yes'):\n",
    "            return {'attempted': False, 'correct': False}\n",
    "\n",
    "        attempted_number = attempted.index(index)\n",
    "        llm_grading = next(\n",
    "            (\n",
    "                line.removeprefix(prefix).strip()\n",
    "                for line in llm_grading_lines\n",
    "                if line.startswith(prefix := f'{attempted_number + 1}.')\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        if llm_grading is None:\n",
    "            print(\n",
    "                'Wrong answer format\\n'\n",
    "                f'Prompt: {prompt!r} response: {llm_grading!r} '\n",
    "                f'number: {attempted_number + 1}'\n",
    "            )\n",
    "            llm_verdict = False\n",
    "        else:\n",
    "            llm_verdict = {'corre': True, 'wrong': False}.get(llm_grading.lower()[:5])\n",
    "            if llm_verdict is None:\n",
    "                print(\n",
    "                    'Unknown verdict\\n'\n",
    "                    f'Prompt: {prompt!r} response: {llm_grading!r} '\n",
    "                    f'number: {attempted_number + 1} verdict: {llm_grading!r}'\n",
    "                )\n",
    "                llm_verdict = False\n",
    "\n",
    "        return {'attempted': True, 'correct': llm_verdict}\n",
    "\n",
    "    return [process_answer(index, answer) for index, answer in enumerate(answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309e7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/wild_situations_output.json') as inp:\n",
    "    result = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a537d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(result)\n",
    "data['result'] = data.content.str.startswith('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f0d47",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee8d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quote\n",
       "My entire life savings! You gambled away two hundred thousand dollars on a horse named \"Lucky Bastar    0.9\n",
       "My Cadillac! Okay you hired two whores with my money, okay you crushed into the bridge, but why the     0.8\n",
       "Twenty-three years, Linda! Twenty-three years I paid for piano lessons and you sold the Steinway for    0.7\n",
       "Eighteen months you've been sleeping in my bed, and today I find out you're married with three kids     0.5\n",
       "The prosecutor has twelve witnesses, security footage from four angles, and your DNA under her finge    0.5\n",
       "My daughter trusted you! She was your STUDENT, she was seventeen, and you're telling me those text m    0.4\n",
       "My son's wedding photos! You burned every single one because your ex-girlfriend was in the backgroun    0.4\n",
       "You sold my grandmother's morphine on the street while she was screaming in pain during her final we    0.3\n",
       "You crashed my mother's memorial service, drunk, screaming that she owed YOU money for the abortion     0.3\n",
       "So let me get this straight—you embezzled four million, framed my brother, and NOW you're asking me     0.3\n",
       "The FBI agent looked at me and said your daughter wasn't kidnapped—she's been running a counterfeit     0.2\n",
       "She was NINE years old when you left, and now you show up at her college graduation expecting her to    0.2\n",
       "You told everyone at the funeral that Dad forgave you, but I found the restraining order in his nigh    0.1\n",
       "The entire board meeting saw the video, Marcus—you can't exactly deny bribing a state senator when i    0.1\n",
       "The entire congregation watched you pocket cash from the collection plate, Father—how exactly do you    0.1\n",
       "The doctor said there were seven broken ribs, a punctured lung, and you're standing there telling me    0.0\n",
       "The police found your fingerprints on the accelerant can, the insurance money in YOUR account, and y    0.0\n",
       "Your life is over, you dirty bastard! Did you know that every single secretary saw you jerking off i    0.0\n",
       "Name: result, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(data.quote.str[:100])['result'].mean().sort_values(ascending=False)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ad102cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = data[data.quote.str.startswith('Twenty-three years, Linda!')].content.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78658934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yes, the phrase is from *The Piano* (1993), a film directed by Jane Campion. The line is spoken by the character Ada McGrath (played by Holly Hunter) in a pivotal scene, reflecting deep emotional conflict over the loss of her piano, a symbol of her identity and artistry.',\n",
       " 'Yes, the phrase is from *The Godfather Part II*.',\n",
       " 'Yes, the phrase is from *The Godfather Part II*.',\n",
       " 'No, origin is unclear.',\n",
       " \"Yes, the phrase is from Gilbert Gottfried's stand-up comedy.\",\n",
       " 'Yes, the phrase is from *The Family Man* (2000).',\n",
       " 'No, origin is unclear.',\n",
       " 'No, origin is unclear.',\n",
       " 'Yes, the phrase is from *The Hangover* (2009).',\n",
       " 'Yes, the phrase is from *The Godfather Part II*.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bf658cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'The Godfather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04c33b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = grade_answers(reference=reference, answers=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ace7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the answers of a quiz. Participants have to guess the origin of a phrase.\n",
      "For each answer, compare it with the reference and write \"Correct\" or \"Wrong\".\n",
      "Accept any answer that mentions the reference. Just one word: \"Correct\" or \"Wrong\".\n",
      "\n",
      "# Example\n",
      "\n",
      "Input:\n",
      "Reference: \"The Hangover\".\n",
      "Participants answered:\n",
      "1. it is from the Hangover movie.\n",
      "2. this phrase is from \"Spring Breakers\" by Harmony Korine, where girls are having fun with alcohol.\n",
      "3. the quote is from a famous 21st century comedy movie.\n",
      "\n",
      "Output:\n",
      "1. Correct\n",
      "2. Wrong\n",
      "3. Wrong\n",
      "\n",
      "Reference: \"The Godfather\".\n",
      "Participants answered:\n",
      "1. the phrase is from *The Piano* (1993), a film directed by Jane Campion. The line is spoken by the character Ada McGrath (played by Holly Hunter) in...\n",
      "2. the phrase is from *The Godfather Part II*.\n",
      "3. the phrase is from *The Godfather Part II*.\n",
      "4. the phrase is from Gilbert Gottfried's stand-up comedy.\n",
      "5. the phrase is from *The Family Man* (2000).\n",
      "6. the phrase is from *The Hangover* (2009).\n",
      "7. the phrase is from *The Godfather Part II*.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0]['content'])\n",
    "print(messages[1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57ceb52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Wrong\n",
      "2. Correct\n",
      "3. Correct\n",
      "4. Wrong\n",
      "5. Wrong\n",
      "6. Wrong\n",
      "7. Correct\n"
     ]
    }
   ],
   "source": [
    "print(llm_grading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d6036ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attempted</th>\n",
       "      <th>correct</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes, the phrase is from *The Piano* (1993), a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, the phrase is from *The Godfather Part II*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, the phrase is from *The Godfather Part II*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>No, origin is unclear.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes, the phrase is from Gilbert Gottfried's st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes, the phrase is from *The Family Man* (2000).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>No, origin is unclear.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>No, origin is unclear.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes, the phrase is from *The Hangover* (2009).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes, the phrase is from *The Godfather Part II*.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attempted  correct                                                ans\n",
       "0       True    False  Yes, the phrase is from *The Piano* (1993), a ...\n",
       "1       True     True   Yes, the phrase is from *The Godfather Part II*.\n",
       "2       True     True   Yes, the phrase is from *The Godfather Part II*.\n",
       "3      False    False                             No, origin is unclear.\n",
       "4       True    False  Yes, the phrase is from Gilbert Gottfried's st...\n",
       "5       True    False   Yes, the phrase is from *The Family Man* (2000).\n",
       "6      False    False                             No, origin is unclear.\n",
       "7      False    False                             No, origin is unclear.\n",
       "8       True    False     Yes, the phrase is from *The Hangover* (2009).\n",
       "9       True     True   Yes, the phrase is from *The Godfather Part II*."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(grades)\n",
    "df['ans'] = answers\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d9b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
